{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.3 선형 회귀 모델 구현하기\n",
    " * 선형 회귀 : 주어진 x와 y값을 가지고 서로 간의 관계를 파악하는 것\n",
    " -> 이 관꼐를 알고나면 새로운 x값이 주어졌을 때, y값을 쉽게 알 수 있다.\n",
    "*     어떤 입력에 대한 출력을 예측하는 것이 머신러닝의 기본! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soual\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_data와 y_data의 상관관계를 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x와 y사이의 상관관계를 설명하기 위한 변수들은 W와 b를 각각 -1.0부터 1.0 사이의 균등분포(uniform distribution)을 가진 무작위 값으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name = \"X\")\n",
    "Y = tf.placeholder(tf.float32, name = \"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X:0\", dtype=float32)\n",
      "Tensor(\"Y:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 플레이스홀더에 이름 부여하기\n",
    "플레이스홀더의 name 매개변수로는 플레이스 홀더의 이름을 설정한다. 위의 플레이스홀더를 출력하면 위와 같이 나온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noname = tf.placeholder(tf.float32)\n",
    "Y_noname = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X_noname)\n",
    "print(Y_noname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name 없이 설정한 플레이스홀더를 출력하면 위와 같이 이름이 자동으로 부여되는 것을 확인할 수 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이는 각각의 텐서에 이름을 주면 어떠한 텐서가 어떻게 사용되고 있는지 쉽게 파악할 수 있다. \n",
    "* 또한 5장에서 배우게 될 일종의 디버깅 도구인 TensorBoard 에서 역시 이름을 출력해 주어 디버깅이 더 쉬울 수 있다. \n",
    "- 변수와 연산 혹은 연산함수에도 이름 지정이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X와 Y의 상관관계를 분석하기 위한 수식을 작성한다.\n",
    "> 이는 W와의 곱과 b와의 합을 통해 X와 Y의 관계를 설명하겠다는 것을 의미한다.\n",
    "> X가 주어졌을 때 Y를 만들어 낼 수 있는 W와 b를 찾아낸다는 뜻이다. \n",
    "\n",
    "* W 는 weight(가중치), b는 bias(편향)이며, 이는 선형회귀와 신경망 학습에 가장 기본이 되는 수식이다. \n",
    "\n",
    "* 이 경우 W와 X는 행렬이 아니므로, tf.matmul이 아닌 * 을 사용하여 연산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss function(손실 함수)는 한 쌍의 데이터(x, y)의 데이터에 대한 손실값을 계산하는 함수이다. \n",
    "> 손실값이란 실제값과 모델로 예측한 값이 얼마나 차이나는 지를 나타낸다.\n",
    "> 손실값이 작을수록, 그 모델이 X와 Y의 관계를 잘 설명하고 있음을 의미한다.\n",
    "> 또한, 주어진 X 값에 대한 Y값을 정확하게 예측 가능하다.\n",
    "\n",
    "* 이 손실을 전체 데이터에 대해 구한 경우 이를 cost(비용)이라 한다.\n",
    "\n",
    "* 학습이란 변수들의 값을 다양하게 넣어 계산하며 이 손실값을 최소화 하는 W와 b의 값을 구하는 것을 의미한다.\n",
    "\n",
    "* 주로 '예측값과 실제값의 거리'를 가장 많이 사용한다.\n",
    "> 손실값 = (예측값 - 실제값)^2 \n",
    "\n",
    "    > 비용 = 모든 데이터에 대한 손실값의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로가 기본 제공하는 gradient descent(경사하강법) 최적화 함수를 이용해 최소화하는 연산 그래프를 작성한다.\n",
    "\n",
    "* 최적화 함수 : W와 b를 변경해가면서 손실값을 최소화하는 가장 최적화된 W와 b 값을 찾아주는 함수\n",
    "\n",
    "> 값을 무작위로 변경하면 시간이 너무 오래걸리며, 학습시간도 예측이 어렵다\n",
    "\n",
    "> 따라서 빠르게 최적화 하기 위해, 다양한 방법을 사용하게 된다.\n",
    "\n",
    "* 경사하강법 : 최적화 방법 중 가장 기본적인 알고리즘\n",
    "\n",
    "> 함수의 기울기를 구하고, 기울기가 낮은 쪽으로 계속 이동시키며 최적의 값을 찾아 나가는 방법\n",
    "\n",
    "* learning rate(학습률) : 학습을 얼마나 '급하게' 할 것인가를 설정하는 값\n",
    "> 최적화 함수의 매개변수\n",
    "\n",
    "> 값이 너무 크게 되면 최적의 손실값을 찾지 못하고 지나치게 됨\n",
    "\n",
    "> 값이 너무 작으면 학습 속도가 매우 느려짐\n",
    "\n",
    "**=> 학습을 진행하는 과정에 영향을 주는 변수 : hyperparameter(하이퍼파라미터)**\n",
    "\n",
    "이 값에 따라 학습속도나 신경망 성능이 크게 달라질 수 있다.\n",
    "\n",
    "이를 잘 튜닝하는 것도 과제!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.6770508 [1.2306402] [-0.30381963]\n",
      "1 0.060257126 [1.1369039] [-0.33531177]\n",
      "2 0.016277857 [1.1432517] [-0.32301098]\n",
      "3 0.015013504 [1.1387545] [-0.31570944]\n",
      "4 0.014294499 [1.135534] [-0.30806935]\n",
      "5 0.013615403 [1.1322634] [-0.3006691]\n",
      "6 0.012968662 [1.1290852] [-0.29344064]\n",
      "7 0.012352659 [1.1259819] [-0.2863866]\n",
      "8 0.011765905 [1.1229534] [-0.27950206]\n",
      "9 0.011206992 [1.1199977] [-0.272783]\n",
      "10 0.010674663 [1.1171131] [-0.2662255]\n",
      "11 0.010167598 [1.1142977] [-0.25982565]\n",
      "12 0.009684631 [1.1115501] [-0.2535796]\n",
      "13 0.009224611 [1.1088685] [-0.2474837]\n",
      "14 0.008786428 [1.1062514] [-0.24153434]\n",
      "15 0.008369083 [1.1036971] [-0.23572803]\n",
      "16 0.00797153 [1.1012044] [-0.23006125]\n",
      "17 0.0075928755 [1.0987715] [-0.22453076]\n",
      "18 0.0072322073 [1.096397] [-0.21913318]\n",
      "19 0.0068886825 [1.0940797] [-0.21386537]\n",
      "20 0.006561464 [1.0918181] [-0.20872419]\n",
      "21 0.00624978 [1.0896109] [-0.20370658]\n",
      "22 0.005952912 [1.0874567] [-0.19880962]\n",
      "23 0.0056701456 [1.0853543] [-0.19403037]\n",
      "24 0.0054008164 [1.0833024] [-0.18936603]\n",
      "25 0.005144259 [1.0812999] [-0.18481377]\n",
      "26 0.004899913 [1.0793455] [-0.18037099]\n",
      "27 0.0046671587 [1.0774381] [-0.17603497]\n",
      "28 0.004445469 [1.0755765] [-0.17180324]\n",
      "29 0.004234304 [1.0737597] [-0.1676732]\n",
      "30 0.0040331804 [1.0719866] [-0.16364244]\n",
      "31 0.0038415864 [1.0702561] [-0.15970857]\n",
      "32 0.0036591121 [1.0685672] [-0.1558693]\n",
      "33 0.0034853003 [1.0669188] [-0.1521223]\n",
      "34 0.0033197503 [1.0653101] [-0.1484654]\n",
      "35 0.003162061 [1.0637401] [-0.14489637]\n",
      "36 0.0030118574 [1.0622079] [-0.14141315]\n",
      "37 0.0028687913 [1.0607125] [-0.13801369]\n",
      "38 0.0027325305 [1.059253] [-0.13469595]\n",
      "39 0.002602725 [1.0578285] [-0.13145794]\n",
      "40 0.0024790934 [1.0564384] [-0.12829778]\n",
      "41 0.0023613365 [1.0550817] [-0.1252136]\n",
      "42 0.002249169 [1.0537575] [-0.12220355]\n",
      "43 0.0021423285 [1.0524653] [-0.11926585]\n",
      "44 0.0020405694 [1.0512041] [-0.1163988]\n",
      "45 0.0019436405 [1.0499731] [-0.11360067]\n",
      "46 0.0018513207 [1.0487719] [-0.11086979]\n",
      "47 0.0017633769 [1.0475994] [-0.10820457]\n",
      "48 0.0016796202 [1.0464551] [-0.10560343]\n",
      "49 0.0015998367 [1.0453384] [-0.1030648]\n",
      "50 0.0015238422 [1.0442485] [-0.1005872]\n",
      "51 0.0014514579 [1.0431848] [-0.09816913]\n",
      "52 0.0013825144 [1.0421467] [-0.09580921]\n",
      "53 0.0013168436 [1.0411335] [-0.09350605]\n",
      "54 0.0012542944 [1.0401447] [-0.09125825]\n",
      "55 0.0011947122 [1.0391797] [-0.08906447]\n",
      "56 0.0011379645 [1.0382378] [-0.08692345]\n",
      "57 0.0010839134 [1.0373186] [-0.08483389]\n",
      "58 0.0010324215 [1.0364214] [-0.08279455]\n",
      "59 0.000983383 [1.035546] [-0.08080421]\n",
      "60 0.00093667646 [1.0346913] [-0.07886176]\n",
      "61 0.00089217984 [1.0338575] [-0.07696594]\n",
      "62 0.00084979954 [1.0330435] [-0.07511574]\n",
      "63 0.00080943597 [1.0322491] [-0.07331001]\n",
      "64 0.0007709881 [1.0314739] [-0.07154766]\n",
      "65 0.00073435926 [1.0307174] [-0.06982768]\n",
      "66 0.00069948105 [1.0299789] [-0.0681491]\n",
      "67 0.0006662551 [1.0292583] [-0.06651083]\n",
      "68 0.00063460536 [1.0285549] [-0.06491196]\n",
      "69 0.0006044637 [1.0278684] [-0.06335154]\n",
      "70 0.0005757489 [1.0271986] [-0.06182859]\n",
      "71 0.00054840144 [1.0265447] [-0.06034229]\n",
      "72 0.0005223536 [1.0259066] [-0.05889171]\n",
      "73 0.00049753924 [1.0252838] [-0.05747599]\n",
      "74 0.00047390573 [1.024676] [-0.05609431]\n",
      "75 0.0004513949 [1.0240828] [-0.05474583]\n",
      "76 0.00042995423 [1.0235039] [-0.05342978]\n",
      "77 0.00040953164 [1.0229388] [-0.05214538]\n",
      "78 0.00039007748 [1.0223874] [-0.05089185]\n",
      "79 0.00037154774 [1.0218493] [-0.04966842]\n",
      "80 0.00035390258 [1.0213239] [-0.04847446]\n",
      "81 0.00033708764 [1.0208114] [-0.04730913]\n",
      "82 0.0003210766 [1.0203111] [-0.04617187]\n",
      "83 0.00030582547 [1.0198228] [-0.04506194]\n",
      "84 0.00029130038 [1.0193462] [-0.04397869]\n",
      "85 0.000277462 [1.0188812] [-0.04292145]\n",
      "86 0.00026427984 [1.0184274] [-0.04188963]\n",
      "87 0.00025172823 [1.0179844] [-0.04088265]\n",
      "88 0.00023977195 [1.017552] [-0.03989988]\n",
      "89 0.00022838374 [1.01713] [-0.03894073]\n",
      "90 0.00021753518 [1.0167183] [-0.0380046]\n",
      "91 0.00020719938 [1.0163164] [-0.03709098]\n",
      "92 0.0001973582 [1.0159242] [-0.03619935]\n",
      "93 0.00018798409 [1.0155413] [-0.03532917]\n",
      "94 0.0001790553 [1.0151677] [-0.03447986]\n",
      "95 0.00017054932 [1.0148032] [-0.03365098]\n",
      "96 0.0001624493 [1.0144472] [-0.03284205]\n",
      "97 0.00015473079 [1.0141] [-0.03205252]\n",
      "98 0.00014738092 [1.013761] [-0.031282]\n",
      "99 0.00014038145 [1.0134302] [-0.03053002]\n",
      "\n",
      "==== Test ====\n",
      "\n",
      "X : 5, Y :  [5.036621]\n",
      "X : 2.5, Y :  [2.5030456]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict = {X : x_data,Y : y_data})\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    print('\\n==== Test ====\\n')\n",
    "    print(\"X : 5, Y : \", sess.run(hypothesis, feed_dict={X:5}))\n",
    "    print(\"X : 2.5, Y : \", sess.run(hypothesis, feed_dict = {X:2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 실행해 학습시키고 결과 확인\n",
    "\n",
    "1. 세션을 생성하고 변수 초기화\n",
    "2. 파이썬의 with 기능을 사용하여 세션 블록을 만들고 세션 종료를 자동으로 처리하도록 만듦\n",
    "3. 최적화를 수행하는 그래프인 train_op를 실행하고, 실행 시마다 변화하는 손실값을 출력하는 코드\n",
    "4. 학습은 100번 수행, feed_dict를 통해 상관관계를 알아내고자 하는 데이터인 x_data와 y_data 입력!\n",
    "\n",
    "**-> 손실값과 변수들의 변화 확인**\n",
    "\n",
    "손실값이 점점 줄어든다면 학습이 정상적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 학습에 사용되지 않은 5와 2.5를 X값으로 넣어 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.223809 [1.0091162] [0.38197625]\n",
      "1 0.16022225 [0.8478173] [0.30193454]\n",
      "2 0.01544565 [0.8690807] [0.30242074]\n",
      "3 0.013073489 [0.8703038] [0.2943043]\n",
      "4 0.01243291 [0.8736319] [0.28732193]\n",
      "5 0.011842108 [0.87664676] [0.2804048]\n",
      "6 0.011279591 [0.87961453] [0.27366513]\n",
      "7 0.010743801 [0.8825083] [0.2670863]\n",
      "8 0.010233462 [0.8853327] [0.26066574]\n",
      "9 0.009747366 [0.88808924] [0.2543995]\n",
      "10 0.00928435 [0.8907795] [0.24828391]\n",
      "11 0.008843341 [0.8934051] [0.24231534]\n",
      "12 0.008423271 [0.89596754] [0.23649023]\n",
      "13 0.008023158 [0.89846843] [0.23080517]\n",
      "14 0.0076420694 [0.9009092] [0.22525676]\n",
      "15 0.007279062 [0.9032913] [0.21984173]\n",
      "16 0.006933298 [0.90561604] [0.21455686]\n",
      "17 0.006603951 [0.90788496] [0.20939906]\n",
      "18 0.0062902756 [0.91009945] [0.20436528]\n",
      "19 0.0059914794 [0.91226053] [0.19945244]\n",
      "20 0.0057068695 [0.91436976] [0.19465776]\n",
      "21 0.0054357895 [0.9164282] [0.1899783]\n",
      "22 0.0051775766 [0.9184372] [0.18541135]\n",
      "23 0.0049316497 [0.92039794] [0.1809542]\n",
      "24 0.004697389 [0.92231154] [0.1766042]\n",
      "25 0.0044742567 [0.9241791] [0.17235874]\n",
      "26 0.0042617316 [0.9260018] [0.16821535]\n",
      "27 0.0040593 [0.9277807] [0.16417158]\n",
      "28 0.0038664814 [0.92951673] [0.16022499]\n",
      "29 0.0036828194 [0.9312111] [0.1563733]\n",
      "30 0.0035078737 [0.9328647] [0.1526142]\n",
      "31 0.003341253 [0.93447864] [0.14894548]\n",
      "32 0.0031825416 [0.9360537] [0.14536493]\n",
      "33 0.0030313684 [0.93759096] [0.14187047]\n",
      "34 0.002887381 [0.9390912] [0.13846]\n",
      "35 0.0027502275 [0.94055545] [0.13513152]\n",
      "36 0.0026195857 [0.9419844] [0.13188304]\n",
      "37 0.0024951512 [0.94337904] [0.12871265]\n",
      "38 0.002376629 [0.9447402] [0.1256185]\n",
      "39 0.0022637427 [0.94606864] [0.12259874]\n",
      "40 0.0021562171 [0.9473651] [0.11965156]\n",
      "41 0.002053787 [0.94863033] [0.11677519]\n",
      "42 0.001956231 [0.9498653] [0.11396801]\n",
      "43 0.001863307 [0.9510705] [0.1112283]\n",
      "44 0.0017748055 [0.9522467] [0.10855445]\n",
      "45 0.0016904976 [0.95339465] [0.10594487]\n",
      "46 0.0016101984 [0.95451504] [0.10339804]\n",
      "47 0.0015337091 [0.9556084] [0.10091241]\n",
      "48 0.0014608604 [0.9566756] [0.09848656]\n",
      "49 0.0013914664 [0.95771706] [0.09611899]\n",
      "50 0.0013253727 [0.95873356] [0.09380837]\n",
      "51 0.0012624171 [0.95972556] [0.09155326]\n",
      "52 0.001202453 [0.9606937] [0.08935239]\n",
      "53 0.0011453349 [0.9616386] [0.08720444]\n",
      "54 0.0010909279 [0.96256083] [0.08510812]\n",
      "55 0.0010391115 [0.9634608] [0.08306216]\n",
      "56 0.0009897532 [0.9643392] [0.08106542]\n",
      "57 0.00094273913 [0.9651965] [0.07911666]\n",
      "58 0.0008979602 [0.96603316] [0.07721476]\n",
      "59 0.00085530523 [0.9668496] [0.07535854]\n",
      "60 0.00081467343 [0.96764654] [0.07354697]\n",
      "61 0.0007759792 [0.9684243] [0.07177897]\n",
      "62 0.0007391188 [0.9691833] [0.07005344]\n",
      "63 0.00070401275 [0.9699242] [0.06836944]\n",
      "64 0.0006705678 [0.9706472] [0.06672588]\n",
      "65 0.00063871616 [0.9713528] [0.06512181]\n",
      "66 0.0006083746 [0.9720414] [0.06355632]\n",
      "67 0.00057947705 [0.97271353] [0.06202847]\n",
      "68 0.0005519528 [0.9733695] [0.06053736]\n",
      "69 0.00052573584 [0.9740097] [0.05908211]\n",
      "70 0.0005007635 [0.97463447] [0.05766182]\n",
      "71 0.00047697593 [0.9752442] [0.05627566]\n",
      "72 0.00045432057 [0.9758394] [0.05492284]\n",
      "73 0.0004327379 [0.97642016] [0.05360252]\n",
      "74 0.0004121843 [0.976987] [0.05231396]\n",
      "75 0.00039260124 [0.9775402] [0.05105635]\n",
      "76 0.00037395427 [0.97808015] [0.04982901]\n",
      "77 0.00035619293 [0.9786071] [0.04863115]\n",
      "78 0.0003392717 [0.9791214] [0.04746208]\n",
      "79 0.00032315738 [0.9796233] [0.04632112]\n",
      "80 0.0003078064 [0.9801131] [0.04520757]\n",
      "81 0.0002931852 [0.9805912] [0.04412081]\n",
      "82 0.00027925937 [0.98105776] [0.04306019]\n",
      "83 0.00026599175 [0.981513] [0.04202503]\n",
      "84 0.00025335953 [0.9819575] [0.0410148]\n",
      "85 0.00024132342 [0.98239124] [0.04002883]\n",
      "86 0.00022986183 [0.9828146] [0.03906658]\n",
      "87 0.00021894278 [0.98322767] [0.03812741]\n",
      "88 0.00020854169 [0.9836309] [0.03721087]\n",
      "89 0.00019863539 [0.98402435] [0.03631633]\n",
      "90 0.00018920004 [0.9844084] [0.03544332]\n",
      "91 0.0001802139 [0.98478323] [0.03459131]\n",
      "92 0.0001716535 [0.985149] [0.03375976]\n",
      "93 0.00016349835 [0.985506] [0.03294818]\n",
      "94 0.0001557337 [0.98585445] [0.03215615]\n",
      "95 0.00014833552 [0.9861945] [0.03138313]\n",
      "96 0.0001412917 [0.9865264] [0.03062873]\n",
      "97 0.00013457984 [0.98685026] [0.02989241]\n",
      "98 0.0001281865 [0.98716635] [0.02917382]\n",
      "99 0.00012209707 [0.9874749] [0.02847253]\n",
      "\n",
      "==== Test=====\n",
      "\n",
      "X : 5, Y :  [4.965847]\n",
      "X : 2.5, Y :  [2.4971597]\n"
     ]
    }
   ],
   "source": [
    "##### 전체 코드\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "X = tf.placeholder(tf.float32, name = 'X')\n",
    "Y = tf.placeholder(tf.float32, name = 'Y')\n",
    "\n",
    "hypothesis = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict = {X:x_data, Y:y_data})\n",
    "        \n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    print(\"\\n==== Test=====\\n\")\n",
    "    print(\"X : 5, Y : \", sess.run(hypothesis, feed_dict = {X:5}))\n",
    "    print(\"X : 2.5, Y : \", sess.run(hypothesis, feed_dict = {X:2.5}))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
